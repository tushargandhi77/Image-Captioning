{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:10.235140Z",
     "start_time": "2024-05-03T17:31:10.221115Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:10.857551Z",
     "start_time": "2024-05-03T17:31:10.238132Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('results.csv',delimiter='|')",
   "id": "780f6650f0230286",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:10.905401Z",
     "start_time": "2024-05-03T17:31:10.859510Z"
    }
   },
   "cell_type": "code",
   "source": "df.sample(10)",
   "id": "d8aee886de0df408",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            image_name  comment_number  \\\n",
       "66122   3335997221.jpg               2   \n",
       "58567   3169213620.jpg               2   \n",
       "32148   2507349105.jpg               3   \n",
       "123626  4888278371.jpg               1   \n",
       "17054   2099852435.jpg               4   \n",
       "155208  8052328504.jpg               3   \n",
       "133577  5331268239.jpg               2   \n",
       "21181   2230458748.jpg               1   \n",
       "129376  4982003362.jpg               1   \n",
       "100378   446907949.jpg               3   \n",
       "\n",
       "                                                  comment  \n",
       "66122    a group of five young people all dressed in b...  \n",
       "58567    Small child in diaper playing with Thomas tra...  \n",
       "32148           A young woman is drinking bottled water .  \n",
       "123626   An Asian woman wearing turquoise stands near ...  \n",
       "17054           Children playing and fishing on a beach .  \n",
       "155208              A football player lands on his head .  \n",
       "133577   A little boy is sitting in a car seat , in th...  \n",
       "21181    Slightly inebriated woman in a maroon apron e...  \n",
       "129376   A young lady with a blue cap , and eyeglasses...  \n",
       "100378         Two people are walking through the woods .  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66122</th>\n",
       "      <td>3335997221.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>a group of five young people all dressed in b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58567</th>\n",
       "      <td>3169213620.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Small child in diaper playing with Thomas tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32148</th>\n",
       "      <td>2507349105.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A young woman is drinking bottled water .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123626</th>\n",
       "      <td>4888278371.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>An Asian woman wearing turquoise stands near ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17054</th>\n",
       "      <td>2099852435.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Children playing and fishing on a beach .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155208</th>\n",
       "      <td>8052328504.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A football player lands on his head .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133577</th>\n",
       "      <td>5331268239.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A little boy is sitting in a car seat , in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21181</th>\n",
       "      <td>2230458748.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Slightly inebriated woman in a maroon apron e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129376</th>\n",
       "      <td>4982003362.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young lady with a blue cap , and eyeglasses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100378</th>\n",
       "      <td>446907949.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Two people are walking through the woods .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:10.921343Z",
     "start_time": "2024-05-03T17:31:10.909376Z"
    }
   },
   "cell_type": "code",
   "source": "df.columns = df.columns.str.strip()",
   "id": "994b6ef3944f3892",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:10.982180Z",
     "start_time": "2024-05-03T17:31:10.923338Z"
    }
   },
   "cell_type": "code",
   "source": "df['image_name'].value_counts()",
   "id": "90b8bf76b98f56d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_name\n",
       "1000092795.jpg    5\n",
       "459538095.jpg     5\n",
       "459804826.jpg     5\n",
       "459778335.jpg     5\n",
       "4597303045.jpg    5\n",
       "                 ..\n",
       "3029715635.jpg    5\n",
       "3029472296.jpg    5\n",
       "3029463004.jpg    5\n",
       "3029411230.jpg    5\n",
       "998845445.jpg     5\n",
       "Name: count, Length: 31783, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.001257Z",
     "start_time": "2024-05-03T17:31:10.984175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_comments = {}\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    comment = row['comment']\n",
    "    if image_name in image_comments:\n",
    "        image_comments[image_name].append(comment)\n",
    "    else:\n",
    "        image_comments[image_name] = [comment]"
   ],
   "id": "c6f3ccacf3dc1d3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.791375Z",
     "start_time": "2024-05-03T17:31:21.004251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.DataFrame(image_comments)\n",
    "data = data.transpose()"
   ],
   "id": "679b31417f515e5b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.807336Z",
     "start_time": "2024-05-03T17:31:21.793369Z"
    }
   },
   "cell_type": "code",
   "source": "data.reset_index(inplace=True)",
   "id": "814027c3b1938940",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.823003Z",
     "start_time": "2024-05-03T17:31:21.809331Z"
    }
   },
   "cell_type": "code",
   "source": "data.columns = ['image','first','second','third','fourth','fifth']",
   "id": "6eaf36be795053ef",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.838126Z",
     "start_time": "2024-05-03T17:31:21.824003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = data.iloc[:25000,:]\n",
    "test_data = data.iloc[25000:,:]"
   ],
   "id": "7073772698e16468",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:21.868988Z",
     "start_time": "2024-05-03T17:31:21.840063Z"
    }
   },
   "cell_type": "code",
   "source": "train_data.head()",
   "id": "6014e7eafa8064e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            image                                              first  \\\n",
       "0  1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
       "1    10002456.jpg   Several men in hard hats are operating a gian...   \n",
       "2  1000268201.jpg   A child in a pink dress is climbing up a set ...   \n",
       "3  1000344755.jpg   Someone in a blue shirt and hat is standing o...   \n",
       "4  1000366164.jpg   Two men , one in a gray shirt , one in a blac...   \n",
       "\n",
       "                                              second  \\\n",
       "0   Two young , White males are outside near many...   \n",
       "1   Workers look down from up above on a piece of...   \n",
       "2   A little girl in a pink dress going into a wo...   \n",
       "3   A man in a blue shirt is standing on a ladder...   \n",
       "4   Two guy cooking and joking around with the ca...   \n",
       "\n",
       "                                               third  \\\n",
       "0   Two men in green shirts are standing in a yard .   \n",
       "1   Two men working on a machine wearing hard hats .   \n",
       "2   A little girl climbing the stairs to her play...   \n",
       "3   A man on a ladder cleans the window of a tall...   \n",
       "4     Two men in a kitchen cooking food on a stove .   \n",
       "\n",
       "                                              fourth  \\\n",
       "0       A man in a blue shirt standing in a garden .   \n",
       "1              Four men on top of a tall structure .   \n",
       "2    A little girl climbing into a wooden playhouse    \n",
       "3   man in blue shirt and jeans on ladder cleanin...   \n",
       "4          Two men are at the stove preparing food .   \n",
       "\n",
       "                                      fifth  \n",
       "0   Two friends enjoy time spent together .  \n",
       "1                Three men on a large rig .  \n",
       "2     A girl going into a wooden building .  \n",
       "3         a man on a ladder cleans a window  \n",
       "4              Two men are cooking a meal .  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "      <th>fourth</th>\n",
       "      <th>fifth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "      <td>Workers look down from up above on a piece of...</td>\n",
       "      <td>Two men working on a machine wearing hard hats .</td>\n",
       "      <td>Four men on top of a tall structure .</td>\n",
       "      <td>Three men on a large rig .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set ...</td>\n",
       "      <td>A little girl in a pink dress going into a wo...</td>\n",
       "      <td>A little girl climbing the stairs to her play...</td>\n",
       "      <td>A little girl climbing into a wooden playhouse</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000344755.jpg</td>\n",
       "      <td>Someone in a blue shirt and hat is standing o...</td>\n",
       "      <td>A man in a blue shirt is standing on a ladder...</td>\n",
       "      <td>A man on a ladder cleans the window of a tall...</td>\n",
       "      <td>man in blue shirt and jeans on ladder cleanin...</td>\n",
       "      <td>a man on a ladder cleans a window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000366164.jpg</td>\n",
       "      <td>Two men , one in a gray shirt , one in a blac...</td>\n",
       "      <td>Two guy cooking and joking around with the ca...</td>\n",
       "      <td>Two men in a kitchen cooking food on a stove .</td>\n",
       "      <td>Two men are at the stove preparing food .</td>\n",
       "      <td>Two men are cooking a meal .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:28.843787Z",
     "start_time": "2024-05-03T17:31:21.870981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ],
   "id": "e33dd99836acf67e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:31:28.859717Z",
     "start_time": "2024-05-03T17:31:28.844762Z"
    }
   },
   "cell_type": "code",
   "source": "folder = 'flickr30k_images'",
   "id": "69373e43691fc9fd",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:29:09.724633Z",
     "start_time": "2024-05-03T18:29:09.499746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(train_data,directory=folder,target_size=(224,224),x_col=['image','first'],y_col='first',class_mode='input')\n",
    "test_generator = test_datagen.flow_from_dataframe(test_data,directory=folder,target_size=(224,224),x_col='image',y_col='first',class_mode='input')"
   ],
   "id": "45c5a2bf3c475230",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All values in column x_col=['image', 'first'] must be strings.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_generator \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_datagen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflow_from_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfolder\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mx_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfirst\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43my_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfirst\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mclass_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m test_generator \u001B[38;5;241m=\u001B[39m test_datagen\u001B[38;5;241m.\u001B[39mflow_from_dataframe(test_data,directory\u001B[38;5;241m=\u001B[39mfolder,target_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m),x_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m,y_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m'\u001B[39m,class_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\GPU_NM\\lib\\site-packages\\keras\\preprocessing\\image.py:1808\u001B[0m, in \u001B[0;36mImageDataGenerator.flow_from_dataframe\u001B[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001B[0m\n\u001B[0;32m   1801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_duplicates\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[0;32m   1802\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1803\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1804\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1805\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m   1806\u001B[0m     )\n\u001B[1;32m-> 1808\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameIterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1809\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1810\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1811\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1812\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1813\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1814\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1815\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1816\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolor_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1817\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1818\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1819\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1820\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1821\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1823\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_to_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_to_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1824\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1825\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1826\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1827\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1828\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidate_filenames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_filenames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1829\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1830\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\GPU_NM\\lib\\site-packages\\keras\\preprocessing\\image.py:968\u001B[0m, in \u001B[0;36mDataFrameIterator.__init__\u001B[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001B[0m\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m=\u001B[39m dtype\n\u001B[0;32m    967\u001B[0m \u001B[38;5;66;03m# check that inputs match the required class_mode\u001B[39;00m\n\u001B[1;32m--> 968\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_col\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    970\u001B[0m     validate_filenames\n\u001B[0;32m    971\u001B[0m ):  \u001B[38;5;66;03m# check which image files are valid and keep them\u001B[39;00m\n\u001B[0;32m    972\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_filter_valid_filepaths(df, x_col)\n",
      "File \u001B[1;32m~\\.conda\\envs\\GPU_NM\\lib\\site-packages\\keras\\preprocessing\\image.py:1030\u001B[0m, in \u001B[0;36mDataFrameIterator._check_params\u001B[1;34m(self, df, x_col, y_col, weight_col, classes)\u001B[0m\n\u001B[0;32m   1028\u001B[0m \u001B[38;5;66;03m# check that filenames/filepaths column values are all strings\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(df[x_col]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m))):\n\u001B[1;32m-> 1030\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   1031\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll values in column x_col=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m must be strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(x_col)\n\u001B[0;32m   1032\u001B[0m     )\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;66;03m# check labels are string if class_mode is binary or sparse\u001B[39;00m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_mode \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n",
      "\u001B[1;31mTypeError\u001B[0m: All values in column x_col=['image', 'first'] must be strings."
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN Encoder",
   "id": "7a6e4a620a82be4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:49:49.584102Z",
     "start_time": "2024-05-03T18:49:49.569141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Embedding, GlobalAveragePooling2D, concatenate, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_self_attention import SeqSelfAttention"
   ],
   "id": "28aa763dc3057057",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:52:11.252671Z",
     "start_time": "2024-05-03T17:52:07.121159Z"
    }
   },
   "cell_type": "code",
   "source": "base_model = VGG19(weights='imagenet', include_top=False,input_shape=(224,224,3))",
   "id": "2cb1fb12deeec5f1",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Image model",
   "id": "adadb978f54544d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:53:58.686006Z",
     "start_time": "2024-05-03T17:53:58.495611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_input = Input(shape=(224,224,3))\n",
    "x = base_model(image_input)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "image_output = Dropout(0.5)(x)\n",
    "image_model = Model(inputs=image_input, outputs=image_output)"
   ],
   "id": "aadf48ab977020a2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define transformer",
   "id": "93c717ea5d96ba04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:03:42.098391Z",
     "start_time": "2024-05-03T18:03:42.075454Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "a56a2c19e151d7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            image                                              first  \\\n",
       "0  1000092795.jpg   Two young guys with shaggy hair look at their...   \n",
       "1    10002456.jpg   Several men in hard hats are operating a gian...   \n",
       "2  1000268201.jpg   A child in a pink dress is climbing up a set ...   \n",
       "3  1000344755.jpg   Someone in a blue shirt and hat is standing o...   \n",
       "4  1000366164.jpg   Two men , one in a gray shirt , one in a blac...   \n",
       "\n",
       "                                              second  \\\n",
       "0   Two young , White males are outside near many...   \n",
       "1   Workers look down from up above on a piece of...   \n",
       "2   A little girl in a pink dress going into a wo...   \n",
       "3   A man in a blue shirt is standing on a ladder...   \n",
       "4   Two guy cooking and joking around with the ca...   \n",
       "\n",
       "                                               third  \\\n",
       "0   Two men in green shirts are standing in a yard .   \n",
       "1   Two men working on a machine wearing hard hats .   \n",
       "2   A little girl climbing the stairs to her play...   \n",
       "3   A man on a ladder cleans the window of a tall...   \n",
       "4     Two men in a kitchen cooking food on a stove .   \n",
       "\n",
       "                                              fourth  \\\n",
       "0       A man in a blue shirt standing in a garden .   \n",
       "1              Four men on top of a tall structure .   \n",
       "2    A little girl climbing into a wooden playhouse    \n",
       "3   man in blue shirt and jeans on ladder cleanin...   \n",
       "4          Two men are at the stove preparing food .   \n",
       "\n",
       "                                      fifth  \n",
       "0   Two friends enjoy time spent together .  \n",
       "1                Three men on a large rig .  \n",
       "2     A girl going into a wooden building .  \n",
       "3         a man on a ladder cleans a window  \n",
       "4              Two men are cooking a meal .  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "      <th>fourth</th>\n",
       "      <th>fifth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "      <td>Workers look down from up above on a piece of...</td>\n",
       "      <td>Two men working on a machine wearing hard hats .</td>\n",
       "      <td>Four men on top of a tall structure .</td>\n",
       "      <td>Three men on a large rig .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set ...</td>\n",
       "      <td>A little girl in a pink dress going into a wo...</td>\n",
       "      <td>A little girl climbing the stairs to her play...</td>\n",
       "      <td>A little girl climbing into a wooden playhouse</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000344755.jpg</td>\n",
       "      <td>Someone in a blue shirt and hat is standing o...</td>\n",
       "      <td>A man in a blue shirt is standing on a ladder...</td>\n",
       "      <td>A man on a ladder cleans the window of a tall...</td>\n",
       "      <td>man in blue shirt and jeans on ladder cleanin...</td>\n",
       "      <td>a man on a ladder cleans a window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000366164.jpg</td>\n",
       "      <td>Two men , one in a gray shirt , one in a blac...</td>\n",
       "      <td>Two guy cooking and joking around with the ca...</td>\n",
       "      <td>Two men in a kitchen cooking food on a stove .</td>\n",
       "      <td>Two men are at the stove preparing food .</td>\n",
       "      <td>Two men are cooking a meal .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:03:58.568438Z",
     "start_time": "2024-05-03T18:03:58.028608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['first'])\n",
    "word_index = tokenizer.word_index"
   ],
   "id": "84e7e77538d5fed6",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:48:07.047423Z",
     "start_time": "2024-05-03T18:48:06.979602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in data['first'])\n",
    "embedding_dim = 100\n",
    "lstm_units = 256"
   ],
   "id": "92a3d86d112b328f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "18813e64bc6cc01a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:49:53.372176Z",
     "start_time": "2024-05-03T18:49:53.075988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "caption_input = Input(shape=(max_length,))\n",
    "x = Embedding(vocab_size,embedding_dim,input_length=max_length)(caption_input)\n",
    "x = LSTM(lstm_units)(x)\n",
    "caption_output = Dense(256, activation='relu')(x)"
   ],
   "id": "d97bb4c392b6d12",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:52:08.603347Z",
     "start_time": "2024-05-03T18:52:08.579786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "caption_model = Model(inputs=caption_input, outputs=caption_output)\n",
    "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "id": "e30ed2c964aa8346",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Combine the model\n",
   "id": "9339bb2c40dc2afe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:52:50.935235Z",
     "start_time": "2024-05-03T18:52:50.727205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combined_input = image_model.input\n",
    "image_features = image_model.output\n",
    "caption_output = caption_model(image_features)\n",
    "inference_model = Model(inputs=combined_input, outputs=caption_output)"
   ],
   "id": "7d9c39010c9e2155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 82) for input KerasTensor(type_spec=TensorSpec(shape=(None, 82), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, 256).\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:57:18.339236Z",
     "start_time": "2024-05-03T18:57:18.324724Z"
    }
   },
   "cell_type": "code",
   "source": "inference_model.compile(loss='categorical_crossentropy', optimizer='adam')",
   "id": "2dcd1475a88b3593",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:57:41.001633Z",
     "start_time": "2024-05-03T18:57:40.973686Z"
    }
   },
   "cell_type": "code",
   "source": "inference_model.summary()",
   "id": "c2544291e2d05bbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 256)               1662860   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,818,572\n",
      "Trainable params: 21,818,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "58c016ff146c81dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
